{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Documents Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import (\n",
    "   PyPDFLoader,\n",
    "   TextLoader,\n",
    "   Docx2txtLoader,\n",
    "   CSVLoader,\n",
    "   UnstructuredExcelLoader\n",
    "   )\n",
    "import os\n",
    "from pathlib import Path\n",
    "def load_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a document and convert it to a list of Langchain Documents.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the document\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: List of Langchain Documents with page_content and metadata\n",
    "    \"\"\"\n",
    "    # Get the file extension\n",
    "    file_extension = Path(file_path).suffix.lower()\n",
    "    \n",
    "    try:\n",
    "        # Choose appropriate loader based on file extension\n",
    "        if file_extension == '.pdf':\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_extension == '.txt':\n",
    "            loader = TextLoader(file_path)\n",
    "        elif file_extension in ['.docx', '.doc']:\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        elif file_extension == '.csv':\n",
    "            loader = CSVLoader(file_path)\n",
    "        elif file_extension in ['.xlsx', '.xls']:\n",
    "            loader = UnstructuredExcelLoader(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {file_extension}\")\n",
    "        \n",
    "        # Load the document\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Add additional metadata\n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                'source': file_path,\n",
    "                'file_type': file_extension,\n",
    "                'file_name': os.path.basename(file_path),\n",
    "                'creation_date': os.path.getctime(file_path),\n",
    "                'last_modified': os.path.getmtime(file_path),\n",
    "                'len':len(doc.page_content)\n",
    "            })\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document {file_path}: {str(e)}\")\n",
    "        return []\n",
    "# Example usage:\n",
    "def load_documents_from_directory(directory_path: str, \n",
    "                               accepted_extensions: Optional[List[str]] = None) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load all documents from a directory.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory\n",
    "        accepted_extensions (List[str], optional): List of accepted file extensions\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: List of all loaded documents\n",
    "    \"\"\"\n",
    "    if accepted_extensions is None:\n",
    "        accepted_extensions = ['.pdf', '.txt', '.docx', '.doc', '.csv', '.xlsx', '.xls']\n",
    "    \n",
    "    all_documents = []\n",
    "    \n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in accepted_extensions):\n",
    "                file_path = os.path.join(root, file)\n",
    "                documents = load_document(file_path)\n",
    "                all_documents.extend(documents)\n",
    "    \n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_document('Attenion_is_all_you_need.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,\n",
    "    chunk_overlap=70,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in data:\n",
    "    # Extend\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=api_key, # removed from the notebook\n",
    ")\n",
    "def call_llm(prompt:str)->str:\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-70b-8192\",\n",
    "        \n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create context\n",
    "n = 10\n",
    "contexts = []\n",
    "for i in range(0, len(docs_processed), n):\n",
    "   batch = docs_processed[i:i+n]\n",
    "   combined_text = \" \".join([doc.page_content for doc in batch])\n",
    "   contexts.append(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231cdf1ceae447e4a14c50167ce5fc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "import prompts\n",
    "\n",
    "outputs = []\n",
    "for context in tqdm(contexts):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(prompts.QA_generation_prompt.format(context=context,n=10))\n",
    "    try:\n",
    "        for QAs in output_QA_couple.split('Output:::')[1:]:\n",
    "            \n",
    "            question = QAs.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "            answer = QAs.split(\"Answer: \")[-1]\n",
    "            outputs.append(\n",
    "                {\n",
    "                    \"context\": context,\n",
    "                    \"question\": question.strip(),\n",
    "                    \"answer\": answer.strip(),\n",
    "                }\n",
    "            )\n",
    "            print(len(outputs))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(outputs).to_json('output/QAs.json',orient='records',indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "\n",
    "Your task is to evaluate the question based on three criteria and provide ratings for each:\n",
    "\n",
    "1. GROUNDEDNESS (1-5): How well can one answer the question unambiguously with the given context?\n",
    "   - 1: Question cannot be answered at all from the context\n",
    "   - 5: Question can be clearly and unambiguously answered from the context\n",
    "\n",
    "2. RELEVANCE (1-5): How useful is this question for machine learning developers building NLP applications?\n",
    "   - 1: Question is not useful at all\n",
    "   - 5: Question is extremely useful\n",
    "\n",
    "Note:\n",
    "- DO NOT add any extra text outside the specified format.\n",
    "- DO NOT include bold formatting, asterisks, or additional prefixes like \"Answer:::\" unless explicitly mentioned.\n",
    "- Follow the format exactly as shown below:\n",
    "### Question: the question\n",
    "### Context: the context\n",
    "Answer:::\n",
    "Groundedness Evaluation: (your rationale)\n",
    "Groundedness Rating: (1-5)\n",
    "Relevance Evaluation: (your rationale)\n",
    "Relevance Rating: (1-5)\n",
    "### Question: {question}\n",
    "### Context: {context}\n",
    "Answer:::\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24137ee423f045b0b11d2f1ac9ea570a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "evaluation_list = []\n",
    "for output in tqdm(outputs):\n",
    "    eval_data = call_llm(critique_prompt.format(context=output[\"context\"], question=output[\"question\"]))\n",
    "    eval_data_parsed = eval_data.split('Answer:::')[-1]\n",
    "    eval_data_list = [ev for ev in eval_data_parsed.split('\\n') if ev]\n",
    "    evaluation_dict = {}\n",
    "    try:\n",
    "        for eval_row in eval_data_list:\n",
    "            evaluation_dict[eval_row.split(':')[0].strip()] = eval_row.split(':')[1].strip()\n",
    "        evaluation_dict.update({'question':output[\"question\"],'answer':output[\"answer\"]})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    evaluation_list.append(evaluation_dict) \n",
    "    print(len(evaluation_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evaluation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Groundedness Evaluation': 'The context provides a detailed explanation of the scaled dot-product attention mechanism, including the formula for computing attention and the reasoning behind the scaling factor of 1/√dk. The question can be answered unambiguously based on the given context.',\n",
       " 'Groundedness Rating': '5',\n",
       " 'Relevance Evaluation': 'This question is extremely relevant to machine learning developers building NLP applications, as it pertains to a key component of the Transformer architecture and its implementation details.',\n",
       " 'Relevance Rating': '5',\n",
       " 'question': 'What is the purpose of the scaling factor of 1/√dk in the Scaled Dot-Product Attention?',\n",
       " 'answer': 'To counteract the effect of large dot products pushing the softmax function into regions with extremely small gradients'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_data = pd.DataFrame(evaluation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_json('output/Critiques.json',orient='records',indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Groundedness Evaluation', 'Groundedness Rating',\n",
       "       'Relevance Evaluation', 'Relevance Rating', 'question', 'answer', ''],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = df_data.loc[\n",
    "    (df_data[\"Groundedness Rating\"] >= '4')\n",
    "    & (df_data[\"Relevance Rating\"] >= '4')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_questions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    return docs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"text-embedding-3-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=embedding_model_name\n",
    "    )    \n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"Faiss_database\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}.faiss\"\n",
    "    try:\n",
    "        if os.path.isdir(index_folder_path):\n",
    "            return FAISS.load_local(\n",
    "                index_folder_path,\n",
    "                embedding_model,\n",
    "                allow_dangerous_deserialization = True,\n",
    "                distance_strategy=DistanceStrategy.COSINE,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            print(\"Index not found, generating it...\")\n",
    "            docs_processed = split_documents(\n",
    "                chunk_size,\n",
    "                langchain_docs,\n",
    "                embedding_model_name,\n",
    "            )\n",
    "            knowledge_index = FAISS.from_documents(\n",
    "                docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "            )\n",
    "            knowledge_index.save_local(index_folder_path)\n",
    "            return knowledge_index\n",
    "            \n",
    "    except (KeyError, Exception) as e:\n",
    "        print(f\"Error loading existing index: {e}\")\n",
    "        print(\"Creating new index...\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"text-embedding-3-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"Creates a FAISS index from the given embedding model and documents.\"\"\"\n",
    "    \n",
    "    # load embedding_model\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=embedding_model_name,\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "    )\n",
    "\n",
    "    # Create index name\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name}\"\n",
    "    index_folder_path = f\"data/indexes/Faiss_database/{index_name}\"\n",
    "    \n",
    "    try:\n",
    "        if os.path.isdir(index_folder_path):\n",
    "            return FAISS.load_local(\n",
    "                index_folder_path,\n",
    "                embedding_model,\n",
    "                allow_dangerous_deserialization=True,\n",
    "                distance_strategy=DistanceStrategy.COSINE,\n",
    "            )\n",
    "    except (KeyError, Exception) as e:\n",
    "        print(f\"Error loading existing index: {e}\")\n",
    "        print(\"Creating new index...\")\n",
    "        \n",
    "    # If loading fails or index doesn't exist, create new index\n",
    "    texts = [doc.page_content for doc in langchain_docs]\n",
    "    metadatas = [doc.metadata for doc in langchain_docs]\n",
    "    \n",
    "    # Create and save new index\n",
    "    vectorstore = FAISS.from_texts(\n",
    "        texts,\n",
    "        embedding_model,\n",
    "        metadatas=metadatas,\n",
    "        distance_strategy=DistanceStrategy.COSINE,\n",
    "    )\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(index_folder_path, exist_ok=True)\n",
    "    \n",
    "    # Save the index\n",
    "    vectorstore.save_local(index_folder_path)\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    num_retrieved_docs: int = 3,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    \n",
    "\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: pd.DataFrame,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    \n",
    "    outputs = []  \n",
    "    \n",
    "    for _, example in tqdm(eval_dataset.iterrows(),total=len(eval_dataset)):\n",
    "        question = example[\"question\"]\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index)\n",
    "        \n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": relevant_docs,  # Don't create a new list comprehension here\n",
    "        }\n",
    "        \n",
    "        outputs.append(result)\n",
    "    \n",
    "    # Write to file once after the loop is complete\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(outputs, f)\n",
    "    \n",
    "    return outputs  # Optionally return the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt_template = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "def evaluate_answers(\n",
    "    answer_path:str,\n",
    "    Rag_Dataset:pd.DataFrame,\n",
    "    eval_chat_model,\n",
    "    evaluation_prompt_template: str,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    for experiment in tqdm(Rag_Dataset,total= len(Rag_Dataset)):\n",
    "        \n",
    "        eval_prompt = evaluation_prompt_template.format(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.predict(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score\"] = score\n",
    "        experiment[f\"eval_feedback\"] = feedback\n",
    "        answers.append(experiment)\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5ced7b80e346d098a26ce93fb16e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "       \n",
    "output_file_name = f\"./output/rag_final_result.json\"\n",
    "print(\"Loading knowledge base embeddings...\")\n",
    "knowledge_index = load_embeddings(\n",
    "    data,\n",
    "    chunk_size=300,\n",
    ")\n",
    "print(\"Running RAG...\")\n",
    "Rag_outputs = run_rag_tests(\n",
    "    eval_dataset=generated_questions,\n",
    "    llm=call_llm,\n",
    "    knowledge_index=knowledge_index,\n",
    "    output_file=output_file_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9097ee8521da42b8b428ef26656ba95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Running evaluation...\")\n",
    "evaluate_answers(\n",
    "    'output/Evaluation_Result.json',\n",
    "    Rag_outputs,\n",
    "    eval_chat_model,\n",
    "    evaluation_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.read_json('output/Evaluation_Result.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.drop(['retrieved_docs'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'true_answer', 'generated_answer', 'eval_score',\n",
       "       'eval_feedback'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final accuracy: 0.6378378378378379\n"
     ]
    }
   ],
   "source": [
    "print('final accuracy:',df_result['eval_score'].apply(lambda x:x/5).sum() / df_result.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
